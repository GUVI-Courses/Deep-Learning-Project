# -*- coding: utf-8 -*-
"""Image_Classification_with_CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xPi3aVMkQnipjYyO0G1MsuVtF35aHZML

### Dataset Preparation
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from tensorflow.keras.datasets import cifar10
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# load dataset
(X_train, y_train), (X_test, y_test) = cifar10.load_data()
classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

# basic info
print('X_train shape:', X_train.shape)
print('y_train shape:', y_train.shape)
print('X_test shape:', X_test.shape)
print('y_test shape:', y_test.shape)
print(f"Unique labels: {np.unique(y_train)}")

# display sample images from each class
fig, ax = plt.subplots(5, 5, figsize=(10, 10))
for i in range(5):
    for j in range(5):
        idx = np.random.randint(0, len(X_train))
        ax[i, j].set_title(classes[y_train[idx][0]])
        ax[i, j].imshow(X_train[idx])

# normalize data
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# data augmentation
datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    zoom_range=0.1
)

datagen.fit(X_train)

# show augmented images
fig, ax = plt.subplots(5, 5, figsize=(10, 10))
for i in range(5):
    for j in range(5):
        idx = np.random.randint(0, len(X_train))
        ax[i, j].set_title(classes[y_train[idx][0]])
        ax[i, j].imshow(X_train[idx])
        augmented_images = datagen.flow(X_train[idx].reshape(1, 32, 32, 3), batch_size=1)
        ax[i, j].imshow(augmented_images[0].reshape(32, 32, 3))
        ax[i, j].axis('off')

"""### Model Development"""

from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.models import Sequential

model = Sequential([
    # block-1
    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),
    BatchNormalization(),
    Conv2D(32, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.2),

    # block-2
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.25),

    # fully connected block
    Flatten(),
    Dense(512, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(len(np.unique(y_train)), activation='softmax')

])

# model summary
model.summary()

"""### Model Training"""

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping

# compile our model
model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# define callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)
lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)

# train the model using data
history = model.fit(datagen.flow(X_train, y_train, batch_size=32),
                    epochs=30,
                    validation_data=(X_test, y_test),
                    callbacks=[early_stopping, lr_reduce]
)

# visualize the model perfomance
# accuracy graphs from history
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# loss graphs from history
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""### Evaluation of the Model"""

from sklearn.metrics import confusion_matrix, classification_report

# predict on test data
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)

y_pred

# confustion Matix
cm = confusion_matrix(y_test, y_pred)

cm

plt.figure(figsize=(10, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# classification report
print(classification_report(y_test, y_pred, target_names=classes))

# show misclassified images
misclassified_idx = np.where(y_pred != y_test.flatten())[0]
plt.figure(figsize=(10, 10))
for i, idx in enumerate(misclassified_idx[:25]):
    plt.subplot(5, 5, i+1)
    plt.imshow(X_test[idx])
    plt.title(f"Actual: {classes[y_test[idx][0]]}\nPredicted: {classes[y_pred[idx]]}")
    plt.axis('off')
plt.show()

model.save('cifar10_model.keras')

"""### MNIST Fashion Dataset"""

import tensorflow_datasets as tfds

# list datasets
print(tfds.list_builders()[:10])

# load dataset
(ds_train, ds_test), ds_info = tfds.load('fashion_mnist', split=['train', 'test'], as_supervised=True, with_info=True)

ds_info

# preprocessing
def normalize_img(image, label):
    return tf.cast(image, tf.float32) / 255.0, label

ds_train = ds_train.map(normalize_img).batch(128).prefetch(tf.data.AUTOTUNE)
ds_test = ds_test.map(normalize_img).batch(128).prefetch(tf.data.AUTOTUNE)

# visualize the samples
for images, labels in ds_train.take(1):
    fig, ax = plt.subplots(5, 5, figsize=(10, 10))
    for i in range(5):
        for j in range(5):
            idx = np.random.randint(0, 128)
            ax[i, j].imshow(images[idx].numpy().squeeze(), cmap='gray')
            ax[i, j].axis('off')
            ax[i, j].set_title(f"Label: {labels[idx].numpy()}")

# loss function and optimiser
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam()

# metrics
train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')
test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')

"""### Custom Training Loop"""

classes = [labels for labels in ds_info.features['label'].names]

classes

model = Sequential([
    # block-1
    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),
    BatchNormalization(),
    Conv2D(32, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.2),

    # block-2
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.25),

    # fully connected block
    Flatten(),
    Dense(512, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(10, activation='softmax')

])

# one epoch training step
@tf.function
def train_step(images, label):
    with tf.GradientTape() as tape:
        predictions = model(images, training=True)
        loss = loss_fn(label, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    train_loss.update_state(loss)
    train_accuracy.update_state(label, predictions)

@tf.function
def test_step(images, label):
    predictions = model(images, training=False)
    t_loss = loss_fn(label, predictions)

    test_loss.update_state(t_loss)
    test_accuracy.update_state(label, predictions)

import time
EPOCHS = 5

for epochs in range(EPOCHS):
    start = time.time()
    train_loss.reset_state()
    train_accuracy.reset_state()
    test_loss.reset_state()
    test_accuracy.reset_state()

    for images, labels in ds_train:
        train_step(images, labels)

    for images, labels in ds_test:
        test_step(images, labels)


    print(f"Epoch {epochs+1}, Time: {time.time() - start:.2f}s, ")
    print(f"Loss: {train_loss.result():.4f}, Accuracy: {train_accuracy.result() * 100:.2f}%")
    print(f"Test Loss: {test_loss.result():.4f}, Test Accuracy: {test_accuracy.result() * 100:.2f}%")

train_loss.reset_state()

!pip install fastapi uvicorn pyngrok -q

!ngrok config add-authtoken 2wobr9gawXALOTFwg24ucbGc7aa_4vHAC6RNwf3p4MKTh3gF4

"""### Webserver using FastAPI"""

from fastapi import FastAPI, UploadFile, File
from fastapi.responses import JSONResponse
from tensorflow.keras.models import load_model
import uvicorn
from PIL import Image
import numpy as np
import io

app = FastAPI()
model = load_model('cifar10_model.keras')
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

def preprocess_image(image) -> np.ndarray:
    image = image.resize((32, 32)).convert('RGB')
    image = np.array(image)
    image = image.astype('float32') / 255.0
    image = np.expand_dims(image, axis=0)
    return image

!pip install python-multipart

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    contents = await file.read()
    image = Image.open(file.file)
    image = preprocess_image(image)
    prediction = model.predict(image)
    predicted_class = class_names[np.argmax(prediction)]
    return {"prediction" : predicted_class}

from pyngrok import ngrok
import nest_asyncio

ngrok_tunnel = ngrok.connect(8000)
print('Public URL:', ngrok_tunnel.public_url)
nest_asyncio.apply()
uvicorn.run(app, port=8000)





